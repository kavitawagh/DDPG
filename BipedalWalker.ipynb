{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BipedalWalker.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNYGvIbke5QV3+mWH6zYXbA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wVz4SDG8hxnu","colab_type":"code","colab":{}},"source":["#%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgq66-IImuW6","colab_type":"code","colab":{}},"source":["from tensorflow.python.client import device_lib\n","print(device_lib.list_local_devices())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PdI6TWxiPNp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818369736,"user_tz":-330,"elapsed":21040,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XdTRY41idKs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592818403654,"user_tz":-330,"elapsed":51691,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"outputId":"0934e05f-12cf-42ec-ebcf-eebc3962abaa"},"source":["!apt-get update > /dev/null 2>&1\n","!apt-get install cmake > /dev/null 2>&1\n","!pip install --upgrade setuptools 2>&1\n","!pip install ez_setup > /dev/null 2>&1\n","!pip install gym[box2d] > /dev/null 2>&1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (47.3.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o5Ov_yFYmVe4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592818406934,"user_tz":-330,"elapsed":3270,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"outputId":"e9f4c77c-8811-449d-9343-4b0e3be38c8c"},"source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7f37d0167390>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"b_rvHmgyimNY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1592818467748,"user_tz":-330,"elapsed":64067,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"outputId":"d5254b3e-d7a3-43bc-fddb-07b94464723e"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K7_Lrd9riwTD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818467751,"user_tz":-330,"elapsed":64056,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["root_dir = \"/content/gdrive/My Drive/\"\n","base_dir = root_dir + 'Colab Notebooks/DDPG/BipedalWalker/'"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ap_nvlVki-Ym","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818467752,"user_tz":-330,"elapsed":64046,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#import gym\n","#from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","#gymlogger.set_level(40) #error only\n","#import tensorflow as tf\n","#import numpy as np\n","#import random\n","#import matplotlib\n","#import matplotlib.pyplot as plt\n","#%matplotlib inline\n","#import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from IPython import display as ipythondisplay\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9uDGNH5i_tZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818467754,"user_tz":-330,"elapsed":64038,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","  env = Monitor(env, './video', force=True)\n","  return env"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"_d1yXFJSjIam","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818469627,"user_tz":-330,"elapsed":65899,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","import gym\n","import random\n","from collections import deque\n","import tensorflow as tf\n","import tensorflow.keras as tfk\n","import tensorflow.keras.layers as tfkl\n","import pickle\n","import time\n","import sys\n","from IPython.core.debugger import set_trace\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"TzVF3YOTJzuj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818469629,"user_tz":-330,"elapsed":65892,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#utils.py\n","class ReplayMemory(object):\n","    def __init__(self, list, max_size):\n","        self.max_size = max_size\n","        self.mem = deque(list, maxlen = self.max_size)\n","        self.cntr = len(self.mem)\n","\n","    def store(self, state, action, reward, next_state, done):\n","        self.mem.append((state, action, reward, next_state, done))\n","        self.cntr += 1\n","\n","    def sample(self, batch_size):\n","        batch_size = min(batch_size, self.cntr)\n","        return batch_size, random.sample(self.mem, batch_size)\n","\n","# Ornstein-Ulhenbeck Process\n","# Taken from #https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n","class OUNoise(object):\n","    def __init__(self, action_dim, mu=0.0, theta=0.15, max_sigma=0.2, min_sigma=0.2, decay_period=100000):\n","        self.mu           = mu\n","        self.theta        = theta\n","        self.sigma        = max_sigma\n","        self.max_sigma    = max_sigma\n","        self.min_sigma    = min_sigma\n","        self.decay_period = decay_period\n","        self.action_dim   = action_dim #action_space.shape[0]\n","        self.reset()\n","        \n","    def reset(self):\n","        self.state = np.ones(self.action_dim) * self.mu\n","        \n","    def evolve_state(self):\n","        x  = self.state\n","        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n","        self.state = x + dx\n","        return self.state\n","    \n","    def get_ounoise(self, t=0):\n","        ou_state = self.evolve_state()\n","        #self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n","        return ou_state"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"ySnv6tMZm3QX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818469630,"user_tz":-330,"elapsed":65884,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#Hyper parameters\n","gamma = 0.99\n","max_mem_len = 1000000 #15000\n","batch_size = 128 #64\n","actor_input_shape = (24, ) #state shape\n","actor_hidden1_n = 600 #400   \n","actor_hidden2_n = 300 \n","actor_output_n = 4 \n","critic_input1_shape = (24, )  #state shape \n","critic_input2_shape = (4, )   #action shape\n","critic_hidden1_n = 600 #300\n","critic_hidden2_n = 300 #200\n","critic_output_n = 1\n","actor_lr = 0.00005 #0.0001 #10e-4\n","critic_lr = 0.0005 #0.001 #10e-3\n","WD = 0.001 #weight decay\n","tau = 0.001"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"0oxfSE5BKXmo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818469631,"user_tz":-330,"elapsed":65874,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#models\n","\n","class Actor(object):\n","    def __init__(self, input_shape, hidden1_n, hidden2_n, output_n, lr):\n","        super(Actor, self).__init__()\n","        self.lr = lr\n","        self.input_shape = input_shape\n","        self.hidden1_n = hidden1_n\n","        self.hidden2_n = hidden2_n\n","        self.output_n = output_n\n","        self.model = self.generate_model(\"actor_w.h5\")\n","        self.target_model = self.generate_model(\"actor_target_w.h5\")\n","        self.target_model.set_weights(self.model.get_weights())\n","        self.optimizer = tfk.optimizers.Adam(learning_rate=self.lr)\n","        #self.epoch_loss_avg = tf.keras.metrics.Mean()\n","        \n","    def generate_model(self, model_filename):\n","      #k_init1 = tfk.initializers.RandomUniform(-1.5e-3, 1.5e-3)\n","      k_init1 = tfk.initializers.GlorotUniform()\n","      k_init2 = tfk.initializers.RandomUniform(-3e-3, 3e-3)\n","\n","      model = tfk.models.Sequential([\n","            tfkl.Input(self.input_shape),\n","            tfkl.BatchNormalization(),\n","            tfkl.Dense(self.hidden1_n, activation='relu', kernel_initializer=k_init1),\n","            tfkl.BatchNormalization(),\n","            tfkl.Dense(self.hidden2_n, activation='relu', kernel_initializer=k_init1),\n","            tfkl.BatchNormalization(),\n","            tfkl.Dense(self.output_n, activation='tanh', kernel_initializer=k_init2) \n","            ])\n","      \n","      try:\n","        model.load_weights(base_dir + model_filename)\n","        print(model_filename, \" loaded.\")\n","      except Exception as e: \n","        print(model_filename, \"not loaded.\")\n","      \n","      return model\n","      \n","    def get_action(self, state, target=False):\n","        if target:\n","            action = self.target_model.predict(state)\n","        else:\n","            action = self.model.predict(state)\n","            #nan_ = np.isnan(action)\n","            #if len(action[nan_]) > 0 : \n","            #  print(\"actor predicted nan action \", action)\n","              #set_trace()\n","        return action    \n","\n","    def get_target_actions(self, states):\n","        actions = self.target_model.predict(states)\n","        return actions #np.clip(action, np.repeat([self.action_space.low], states.shape[0], axis=0), \\\n","                       #np.repeat([self.action_space.high], states.shape[0], axis=0))\n","        \n","    def optimize(self, states, qval_grad):\n","        with tf.GradientTape() as tape:\n","            #tape.watch(states_t)\n","            actions = self.model(states)\n","        policy_grads = tape.gradient(actions, self.model.trainable_weights, \\\n","                                     output_gradients= -1*tf.cast(qval_grad, tf.float32))\n","        #del tape\n","        #set_trace()\n","        #old_w = self.model.get_weights()\n","        self.optimizer.apply_gradients(zip(policy_grads, self.model.trainable_variables))\n","        #nan_ = [tf.reduce_sum(tf.cast(tf.math.is_nan(w), dtype=tf.int32)) for w in self.model.trainable_variables]\n","        #if any(x > 0 for x in nan_) : \n","        #  print(\"update gave nan weight\")\n","        #  set_trace()\n","        #self.epoch_loss_avg.update_state(policy_grads)\n","        return \n","    \n","    def update_target_params(self):\n","        pass\n","        w = self.model.get_weights()\n","        target_w = self.target_model.get_weights()\n","        target_w1 = [tau * w1 + (1 - tau) * w2 for w1, w2 in\n","                          zip(w, target_w)]\n","        self.target_model.set_weights(target_w1)\n","        \n","    def save_models(self):\n","        self.model.save_weights(base_dir + \"actor_w.h5\")\n","        self.target_model.save_weights(base_dir + \"actor_target_w.h5\")\n","        \n","class Critic(object): \n","    def __init__(self, input1_shape, input2_shape, hidden1_n, hidden2_n, output_n, lr):\n","        super(Critic, self).__init__()\n","        self.lr = lr\n","        self.input1_shape = input1_shape\n","        self.input2_shape = input2_shape\n","        self.hidden1_n = hidden1_n\n","        self.hidden2_n = hidden2_n\n","        self.output_n = output_n\n","        self.model = self.generate_model(\"critic_w.h5\")\n","        self.target_model = self.generate_model(\"critic_target_w.h5\")\n","        self.target_model.set_weights(self.model.get_weights())\n","        self.model.compile(tfk.optimizers.Adam(learning_rate=self.lr), loss='mse')\n","        self.optimizer = tfk.optimizers.Adam(learning_rate=self.lr)\n","        self.mse_loss = tf.keras.losses.MeanSquaredError()\n","        #self.epoch_loss_avg = tf.keras.metrics.Mean()\n","        \n","    def generate_model(self, model_filename):\n","        #k_init1 = tfk.initializers.RandomUniform(-1.5e-3, 1.5e-3)\n","        k_init1 = tfk.initializers.GlorotUniform()\n","        k_init2 = tfk.initializers.RandomUniform(-3e-3, 3e-3)\n","\n","        state_input_layer = tfkl.Input(shape=self.input1_shape)\n","        s_bn1 = tfkl.BatchNormalization()(state_input_layer)\n","        s_hidden1 = tfkl.Dense(self.hidden1_n, activation='relu', kernel_initializer=k_init1, \\\n","                               kernel_regularizer=tfk.regularizers.l2(WD))(s_bn1)\n","        s_bn2 = tfkl.BatchNormalization()(s_hidden1)\n","        s_hidden2 = tfkl.Dense(self.hidden1_n, activation='linear', kernel_initializer=k_init1, \\\n","                               kernel_regularizer=tfk.regularizers.l2(WD))(s_bn2)\n","        s_bn3 = tfkl.BatchNormalization()(s_hidden2)\n","\n","        action_input_layer = tfkl.Input(shape=self.input2_shape)\n","        a_hidden1 = tfkl.Dense(self.hidden1_n, activation='relu', kernel_initializer=k_init1, \\\n","                               kernel_regularizer=tfk.regularizers.l2(WD))(action_input_layer)   #linear\n","\n","        hidden_merge = tfkl.Add()([s_bn3, a_hidden1])\n","\n","        hidden3 = tfkl.Dense(self.hidden2_n, activation='relu', kernel_initializer=k_init1, \\\n","                             kernel_regularizer=tfk.regularizers.l2(WD))(hidden_merge)\n","        output_layer = tfkl.Dense(1, activation='linear', kernel_initializer=k_init2, \\\n","                                  kernel_regularizer=tfk.regularizers.l2(WD))(hidden3)\n","        model = tfk.Model(inputs=[state_input_layer, action_input_layer],\n","                      outputs=output_layer)\n","          \n","        try:\n","          model.load_weights(base_dir + model_filename)\n","          print(model_filename, \" loaded.\")\n","        except Exception as e:\n","          print(model_filename, \"not loaded.\")\n","        \n","        #model.compile(loss='mse', optimizer=Adam(lr=self._learning_rate))\n","        #return model, state_input_layer, action_input_layer\n","        return model\n","    \n","    def get_target_qvals(self, states, actions):\n","        #inpt = np.hstack((states, actions))\n","        #return self.target_model.predict(inpt)\n","        return self.target_model.predict([states, actions])\n","    \n","    def optimize(self, states, actions, y):\n","        #self.critic.fit(np.hstack((states, actions)), y, epochs=1, verbose=1)\n","        actions_t = tf.convert_to_tensor(actions)\n","        with tf.GradientTape(persistent=True) as tape:\n","            tape.watch(actions_t)\n","            qvals = self.model([states, actions_t])\n","            loss_value=self.mse_loss(y, qvals)\n","        grads = tape.gradient(loss_value, self.model.trainable_weights)\n","        self.qval_grads = tape.gradient(qvals, actions_t) \n","        del tape\n","        \n","        #set_trace()\n","        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n","        #self.epoch_loss_avg.update_state(loss_value)\n","\n","        #print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n","        #                                  loss_value.numpy()))\n","\n","        #print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n","        #                                  loss(model, features, labels, training=True).numpy()))\n","        return tf.reduce_mean(qvals), loss_value\n","    \n","    def update_target_params(self):\n","        w = self.model.get_weights()\n","        target_w = self.target_model.get_weights()\n","        target_w1 = [tau * w1 + (1 - tau) * w2 for w1, w2 in\n","                          zip(w, target_w)]\n","        self.target_model.set_weights(target_w1)\n","    \n","    def save_models(self):\n","        self.model.save_weights(base_dir + \"critic_w.h5\")\n","        self.target_model.save_weights(base_dir + \"critic_target_w.h5\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"k9CUMbJwm-ec","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592818479372,"user_tz":-330,"elapsed":1899,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#agent\n","\n","class ActorCritic(object):\n","    def __init__(self, action_space):\n","        super(ActorCritic, self).__init__()\n","        self.gamma = gamma\n","        self.actor = Actor(actor_input_shape, actor_hidden1_n, actor_hidden2_n, actor_output_n, actor_lr)\n","        self.critic = Critic(critic_input1_shape, critic_input2_shape, critic_hidden1_n, \\\n","                             critic_hidden2_n, critic_output_n, critic_lr)\n","        self.max_mem_len = max_mem_len        \n","        self.batch_size = batch_size\n","        self.action_space = action_space\n","        self.noise = OUNoise(action_space.shape[0])\n","        try:\n","          self.reward_list = pickle.load(open(base_dir+\"reward_list.file\", \"rb\"))\n","          self.mem = pickle.load(open(base_dir+\"replay_mem.file\", \"rb\"))          \n","        except Exception as e:\n","          self.mem = ReplayMemory([], max_mem_len)\n","          self.reward_list = []\n","\n","    def get_action_noisy(self, state):\n","        action = self.actor.get_action(state[np.newaxis, :], target=False).squeeze(axis=0)\n","        return np.clip(action + self.noise.get_ounoise(), self.action_space.low, self.action_space.high)\n","        \n","    def train(self):\n","        batch_size1, states, actions, rewards, next_states, dones = self.sample_transitions()\n","        #if batch_size1 < self.batch_size: return\n","        \n","        target_actions = self.actor.get_target_actions(next_states)\n","        target_qvals = self.critic.get_target_qvals(next_states, target_actions)\n","        y = rewards + gamma*target_qvals*(1-dones)\n","        qval, qloss = self.critic.optimize(states, actions, y)\n","        self.actor.optimize(states, self.critic.qval_grads)\n","        self.critic.update_target_params()\n","        self.actor.update_target_params()        \n","        return qval, qloss\n","        #return tf.reshape(qval, []), tf.reshape(qloss, [])\n","        \n","\n","    def store_transition(self, state, action, reward, next_state, done):\n","        self.mem.store(state, action, reward, next_state, float(done))\n","\n","    def sample_transitions(self):\n","        batch_size1, samples = self.mem.sample(self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*samples)\n","        states = np.stack(states)\n","        actions = np.stack(actions)\n","        rewards = np.stack(rewards)\n","        next_states = np.stack(next_states)\n","        dones = np.stack(dones)\n","        return batch_size1, states, actions, rewards, next_states, dones\n","\n","    def save_models(self):\n","        self.actor.save_models()\n","        self.critic.save_models()\n","        with open(base_dir+\"reward_list.file\", \"wb\") as f:\n","          pickle.dump(self.reward_list, f, pickle.HIGHEST_PROTOCOL)\n","\n","    def append_reward(self, s, rp, rn):\n","      self.reward_list.append((s, rp, rn))\n","\n","    def save_replay_mem(self):\n","      with open(base_dir+\"replay_mem.file\", \"wb\") as f:\n","        pickle.dump(self.mem, f, pickle.HIGHEST_PROTOCOL)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSmrCmA4jTkU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"244a82f5-d08b-404d-e227-99f831b2fce0"},"source":["env = gym.make ('BipedalWalker-v3') \n","#env = wrap_env(gym.make('BipedalWalker-v3'))\n","agent = ActorCritic(env.action_space)\n","no_of_episodes = 6000\n","epi_start = 119\n","for episode in range(epi_start, no_of_episodes+1):\n","    state = env.reset()\n","    done = False\n","    epoch = 0\n","    tot_rewardp = 0\n","    tot_rewardn = 0\n","    tot_qval = 0\n","    tot_qloss = 0\n","    reward = 0\n","    while not done:\n","        epoch += 1\n","        action = agent.get_action_noisy(state)\n","        #action = env.action_space.sample()\n","        next_state, reward, done, info = env.step(action)\n","        agent.store_transition(state, action, reward, next_state, done)\n","        state = next_state\n","        qval, qloss = agent.train()\n","        tot_qval += qval\n","        tot_qloss += qloss\n","        if reward >= 0: tot_rewardp += reward\n","        else: tot_rewardn += reward\n","    print(\"Epi: {}\\tSteps: {}\\tQV: {:.3e}\\tQL: {:.3e}\\tPR: {:.3f}\\tNR: {:.3f}\\tR: {:.3f}\\tLR: {:.3f}\".format(\\\n","                          episode, epoch, tot_qval/epoch, tot_qloss/epoch, tot_rewardp, tot_rewardn, tot_rewardp+tot_rewardn-reward, reward))\n","    agent.append_reward(epoch, tot_rewardp, tot_rewardn)\n","    if episode % 10 == 0 :\n","        agent.save_models()\n","    if episode % 100 == 0 :\n","        agent.save_replay_mem()\n","        \n","env.close()\n","#agent.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"],"name":"stderr"},{"output_type":"stream","text":["actor_w.h5  loaded.\n","actor_target_w.h5  loaded.\n","critic_w.h5  loaded.\n","critic_target_w.h5  loaded.\n","Epi: 119\tSteps: 88\tQV: -1.450e+01\tQL: 5.971e+01\tPR: 2.609\tNR: -116.024\tR: -13.415\tLR: -100.000\n","Epi: 120\tSteps: 141\tQV: -1.457e+01\tQL: 5.883e+01\tPR: 2.138\tNR: -117.296\tR: -15.158\tLR: -100.000\n","Epi: 121\tSteps: 110\tQV: -1.467e+01\tQL: 6.059e+01\tPR: 0.733\tNR: -115.691\tR: -14.958\tLR: -100.000\n","Epi: 122\tSteps: 197\tQV: -1.474e+01\tQL: 5.649e+01\tPR: 0.260\tNR: -137.126\tR: -36.867\tLR: -100.000\n","Epi: 123\tSteps: 175\tQV: -1.475e+01\tQL: 5.071e+01\tPR: 2.323\tNR: -112.793\tR: -10.470\tLR: -100.000\n","Epi: 124\tSteps: 1600\tQV: -1.532e+01\tQL: 5.332e+01\tPR: 18.872\tNR: -133.196\tR: -114.389\tLR: 0.065\n","Epi: 125\tSteps: 1600\tQV: -1.617e+01\tQL: 5.219e+01\tPR: 11.372\tNR: -128.042\tR: -116.574\tLR: -0.096\n","Epi: 126\tSteps: 1600\tQV: -1.702e+01\tQL: 5.050e+01\tPR: 34.244\tNR: -96.902\tR: -62.606\tLR: -0.052\n","Epi: 127\tSteps: 1600\tQV: -1.784e+01\tQL: 4.794e+01\tPR: 31.041\tNR: -131.591\tR: -100.386\tLR: -0.165\n","Epi: 128\tSteps: 648\tQV: -1.841e+01\tQL: 4.720e+01\tPR: 6.024\tNR: -177.262\tR: -71.237\tLR: -100.000\n","Epi: 129\tSteps: 1074\tQV: -1.890e+01\tQL: 4.856e+01\tPR: 7.260\tNR: -213.816\tR: -106.556\tLR: -100.000\n","Epi: 130\tSteps: 1600\tQV: -1.943e+01\tQL: 4.307e+01\tPR: 20.804\tNR: -129.674\tR: -108.976\tLR: 0.106\n","Epi: 131\tSteps: 1600\tQV: -2.010e+01\tQL: 4.249e+01\tPR: 29.292\tNR: -133.108\tR: -103.783\tLR: -0.033\n","Epi: 132\tSteps: 1600\tQV: -2.080e+01\tQL: 4.315e+01\tPR: 34.561\tNR: -117.839\tR: -83.148\tLR: -0.130\n","Epi: 133\tSteps: 1600\tQV: -2.148e+01\tQL: 4.152e+01\tPR: 45.102\tNR: -116.072\tR: -70.922\tLR: -0.048\n","Epi: 134\tSteps: 1600\tQV: -2.207e+01\tQL: 3.903e+01\tPR: 31.301\tNR: -125.196\tR: -93.790\tLR: -0.105\n","Epi: 135\tSteps: 1600\tQV: -2.270e+01\tQL: 3.936e+01\tPR: 27.931\tNR: -114.657\tR: -86.636\tLR: -0.090\n","Epi: 136\tSteps: 1600\tQV: -2.324e+01\tQL: 3.642e+01\tPR: 41.643\tNR: -115.696\tR: -73.932\tLR: -0.121\n","Epi: 137\tSteps: 1600\tQV: -2.382e+01\tQL: 3.721e+01\tPR: 27.634\tNR: -115.391\tR: -87.680\tLR: -0.077\n","Epi: 138\tSteps: 1600\tQV: -2.434e+01\tQL: 3.493e+01\tPR: 33.287\tNR: -123.454\tR: -90.176\tLR: 0.009\n","Epi: 139\tSteps: 1600\tQV: -2.481e+01\tQL: 3.450e+01\tPR: 35.458\tNR: -109.997\tR: -74.524\tLR: -0.015\n","Epi: 140\tSteps: 1600\tQV: -2.531e+01\tQL: 3.351e+01\tPR: 30.242\tNR: -97.025\tR: -66.797\tLR: 0.014\n","Epi: 141\tSteps: 39\tQV: -2.539e+01\tQL: 2.599e+01\tPR: 0.045\tNR: -112.643\tR: -12.598\tLR: -100.000\n","Epi: 142\tSteps: 39\tQV: -2.539e+01\tQL: 2.182e+01\tPR: 0.000\tNR: -112.375\tR: -12.375\tLR: -100.000\n","Epi: 143\tSteps: 1600\tQV: -2.575e+01\tQL: 3.106e+01\tPR: 20.472\tNR: -126.569\tR: -106.112\tLR: 0.016\n","Epi: 144\tSteps: 1600\tQV: -2.619e+01\tQL: 3.330e+01\tPR: 16.880\tNR: -115.545\tR: -98.612\tLR: -0.053\n","Epi: 145\tSteps: 626\tQV: -2.653e+01\tQL: 3.302e+01\tPR: 9.697\tNR: -145.076\tR: -35.379\tLR: -100.000\n","Epi: 146\tSteps: 1600\tQV: -2.679e+01\tQL: 3.101e+01\tPR: 14.778\tNR: -129.081\tR: -114.204\tLR: -0.099\n","Epi: 147\tSteps: 687\tQV: -2.705e+01\tQL: 2.901e+01\tPR: 15.685\tNR: -155.440\tR: -39.755\tLR: -100.000\n","Epi: 148\tSteps: 382\tQV: -2.717e+01\tQL: 2.852e+01\tPR: 7.773\tNR: -130.265\tR: -22.492\tLR: -100.000\n","Epi: 149\tSteps: 87\tQV: -2.725e+01\tQL: 3.092e+01\tPR: 1.174\tNR: -110.317\tR: -9.143\tLR: -100.000\n","Epi: 150\tSteps: 144\tQV: -2.729e+01\tQL: 2.935e+01\tPR: 10.022\tNR: -104.259\tR: 5.763\tLR: -100.000\n","Epi: 151\tSteps: 754\tQV: -2.737e+01\tQL: 2.937e+01\tPR: 5.122\tNR: -183.549\tR: -78.427\tLR: -100.000\n","Epi: 152\tSteps: 645\tQV: -2.754e+01\tQL: 2.832e+01\tPR: 3.032\tNR: -173.587\tR: -70.556\tLR: -100.000\n","Epi: 153\tSteps: 77\tQV: -2.762e+01\tQL: 2.807e+01\tPR: 4.155\tNR: -107.076\tR: -2.921\tLR: -100.000\n","Epi: 154\tSteps: 310\tQV: -2.767e+01\tQL: 3.068e+01\tPR: 3.737\tNR: -125.556\tR: -21.819\tLR: -100.000\n","Epi: 155\tSteps: 948\tQV: -2.779e+01\tQL: 2.731e+01\tPR: 17.399\tNR: -182.579\tR: -65.180\tLR: -100.000\n","Epi: 156\tSteps: 102\tQV: -2.796e+01\tQL: 3.229e+01\tPR: 4.659\tNR: -103.430\tR: 1.229\tLR: -100.000\n","Epi: 157\tSteps: 100\tQV: -2.790e+01\tQL: 2.648e+01\tPR: 4.958\tNR: -105.678\tR: -0.719\tLR: -100.000\n","Epi: 158\tSteps: 98\tQV: -2.816e+01\tQL: 3.935e+01\tPR: 8.613\tNR: -102.722\tR: 5.890\tLR: -100.000\n","Epi: 159\tSteps: 1087\tQV: -2.815e+01\tQL: 2.974e+01\tPR: 12.326\tNR: -189.412\tR: -77.086\tLR: -100.000\n","Epi: 160\tSteps: 88\tQV: -2.835e+01\tQL: 3.201e+01\tPR: 5.657\tNR: -104.317\tR: 1.339\tLR: -100.000\n","Epi: 161\tSteps: 1574\tQV: -2.847e+01\tQL: 2.863e+01\tPR: 19.217\tNR: -197.232\tR: -78.015\tLR: -100.000\n","Epi: 162\tSteps: 1600\tQV: -2.880e+01\tQL: 2.706e+01\tPR: 8.722\tNR: -125.312\tR: -116.428\tLR: -0.162\n","Epi: 163\tSteps: 1361\tQV: -2.908e+01\tQL: 2.616e+01\tPR: 5.726\tNR: -223.788\tR: -118.062\tLR: -100.000\n","Epi: 164\tSteps: 67\tQV: -2.905e+01\tQL: 1.827e+01\tPR: 4.005\tNR: -104.021\tR: -0.017\tLR: -100.000\n","Epi: 165\tSteps: 739\tQV: -2.928e+01\tQL: 2.610e+01\tPR: 0.388\tNR: -176.915\tR: -76.527\tLR: -100.000\n","Epi: 166\tSteps: 1600\tQV: -2.951e+01\tQL: 2.702e+01\tPR: 4.249\tNR: -112.279\tR: -107.999\tLR: -0.031\n","Epi: 167\tSteps: 898\tQV: -2.973e+01\tQL: 2.474e+01\tPR: 2.583\tNR: -173.395\tR: -70.812\tLR: -100.000\n","Epi: 168\tSteps: 118\tQV: -2.993e+01\tQL: 2.939e+01\tPR: 9.888\tNR: -106.180\tR: 3.708\tLR: -100.000\n","Epi: 169\tSteps: 40\tQV: -2.994e+01\tQL: 2.328e+01\tPR: 0.116\tNR: -105.997\tR: -5.881\tLR: -100.000\n","Epi: 170\tSteps: 1600\tQV: -2.999e+01\tQL: 2.492e+01\tPR: 16.120\tNR: -102.166\tR: -85.960\tLR: -0.086\n","Epi: 171\tSteps: 1600\tQV: -3.027e+01\tQL: 2.482e+01\tPR: 14.164\tNR: -95.718\tR: -81.493\tLR: -0.062\n","Epi: 172\tSteps: 1600\tQV: -3.056e+01\tQL: 2.473e+01\tPR: 13.238\tNR: -103.435\tR: -90.105\tLR: -0.092\n","Epi: 173\tSteps: 1600\tQV: -3.082e+01\tQL: 2.420e+01\tPR: 38.294\tNR: -112.743\tR: -74.385\tLR: -0.064\n","Epi: 174\tSteps: 64\tQV: -3.088e+01\tQL: 1.883e+01\tPR: 4.615\tNR: -104.221\tR: 0.394\tLR: -100.000\n","Epi: 175\tSteps: 1600\tQV: -3.110e+01\tQL: 2.297e+01\tPR: 51.374\tNR: -127.940\tR: -76.610\tLR: 0.045\n","Epi: 176\tSteps: 70\tQV: -3.124e+01\tQL: 2.239e+01\tPR: 3.805\tNR: -104.419\tR: -0.614\tLR: -100.000\n","Epi: 177\tSteps: 55\tQV: -3.120e+01\tQL: 1.939e+01\tPR: 3.931\tNR: -103.846\tR: 0.085\tLR: -100.000\n","Epi: 178\tSteps: 1600\tQV: -3.138e+01\tQL: 2.210e+01\tPR: 42.002\tNR: -137.229\tR: -95.300\tLR: 0.073\n","Epi: 179\tSteps: 45\tQV: -3.143e+01\tQL: 2.180e+01\tPR: 1.690\tNR: -103.911\tR: -2.221\tLR: -100.000\n","Epi: 180\tSteps: 64\tQV: -3.149e+01\tQL: 2.237e+01\tPR: 4.199\tNR: -103.975\tR: 0.225\tLR: -100.000\n","Epi: 181\tSteps: 54\tQV: -3.149e+01\tQL: 2.060e+01\tPR: 2.765\tNR: -105.102\tR: -2.337\tLR: -100.000\n","Epi: 182\tSteps: 57\tQV: -3.153e+01\tQL: 2.297e+01\tPR: 2.555\tNR: -104.562\tR: -2.007\tLR: -100.000\n","Epi: 183\tSteps: 85\tQV: -3.163e+01\tQL: 2.542e+01\tPR: 5.754\tNR: -105.585\tR: 0.169\tLR: -100.000\n","Epi: 184\tSteps: 51\tQV: -3.150e+01\tQL: 1.829e+01\tPR: 0.996\tNR: -103.804\tR: -2.809\tLR: -100.000\n","Epi: 185\tSteps: 65\tQV: -3.154e+01\tQL: 1.888e+01\tPR: 1.394\tNR: -104.856\tR: -3.463\tLR: -100.000\n","Epi: 186\tSteps: 1600\tQV: -3.174e+01\tQL: 2.272e+01\tPR: 30.347\tNR: -138.465\tR: -108.026\tLR: -0.092\n","Epi: 187\tSteps: 55\tQV: -3.190e+01\tQL: 2.081e+01\tPR: 2.234\tNR: -104.292\tR: -2.059\tLR: -100.000\n","Epi: 188\tSteps: 81\tQV: -3.190e+01\tQL: 2.232e+01\tPR: 4.554\tNR: -105.460\tR: -0.906\tLR: -100.000\n","Epi: 189\tSteps: 72\tQV: -3.194e+01\tQL: 2.301e+01\tPR: 4.074\tNR: -105.724\tR: -1.651\tLR: -100.000\n","Epi: 190\tSteps: 48\tQV: -3.193e+01\tQL: 2.491e+01\tPR: 0.080\tNR: -105.250\tR: -5.169\tLR: -100.000\n","Epi: 191\tSteps: 93\tQV: -3.202e+01\tQL: 2.632e+01\tPR: 4.614\tNR: -106.538\tR: -1.924\tLR: -100.000\n","Epi: 192\tSteps: 67\tQV: -3.194e+01\tQL: 2.095e+01\tPR: 3.125\tNR: -103.767\tR: -0.643\tLR: -100.000\n","Epi: 193\tSteps: 68\tQV: -3.194e+01\tQL: 2.185e+01\tPR: 4.130\tNR: -105.499\tR: -1.369\tLR: -100.000\n","Epi: 194\tSteps: 60\tQV: -3.193e+01\tQL: 2.225e+01\tPR: 4.732\tNR: -105.288\tR: -0.556\tLR: -100.000\n","Epi: 195\tSteps: 183\tQV: -3.202e+01\tQL: 2.342e+01\tPR: 0.943\tNR: -130.423\tR: -29.479\tLR: -100.000\n","Epi: 196\tSteps: 128\tQV: -3.210e+01\tQL: 2.377e+01\tPR: 0.000\tNR: -131.566\tR: -31.566\tLR: -100.000\n","Epi: 197\tSteps: 90\tQV: -3.203e+01\tQL: 2.262e+01\tPR: 1.499\tNR: -106.066\tR: -4.567\tLR: -100.000\n","Epi: 198\tSteps: 46\tQV: -3.201e+01\tQL: 1.676e+01\tPR: 0.587\tNR: -105.146\tR: -4.559\tLR: -100.000\n","Epi: 199\tSteps: 74\tQV: -3.204e+01\tQL: 1.854e+01\tPR: 3.437\tNR: -105.025\tR: -1.589\tLR: -100.000\n","Epi: 200\tSteps: 62\tQV: -3.214e+01\tQL: 2.421e+01\tPR: 3.078\tNR: -105.377\tR: -2.299\tLR: -100.000\n","Epi: 201\tSteps: 63\tQV: -3.202e+01\tQL: 1.806e+01\tPR: 3.558\tNR: -105.439\tR: -1.881\tLR: -100.000\n","Epi: 202\tSteps: 65\tQV: -3.206e+01\tQL: 1.789e+01\tPR: 2.222\tNR: -106.354\tR: -4.133\tLR: -100.000\n","Epi: 203\tSteps: 65\tQV: -3.217e+01\tQL: 2.787e+01\tPR: 3.573\tNR: -105.556\tR: -1.983\tLR: -100.000\n","Epi: 204\tSteps: 73\tQV: -3.211e+01\tQL: 2.284e+01\tPR: 3.449\tNR: -106.739\tR: -3.291\tLR: -100.000\n","Epi: 205\tSteps: 61\tQV: -3.213e+01\tQL: 2.249e+01\tPR: 3.141\tNR: -104.848\tR: -1.707\tLR: -100.000\n","Epi: 206\tSteps: 49\tQV: -3.220e+01\tQL: 2.479e+01\tPR: 0.993\tNR: -105.111\tR: -4.118\tLR: -100.000\n","Epi: 207\tSteps: 56\tQV: -3.205e+01\tQL: 1.603e+01\tPR: 2.202\tNR: -105.094\tR: -2.892\tLR: -100.000\n","Epi: 208\tSteps: 44\tQV: -3.214e+01\tQL: 2.032e+01\tPR: 0.570\tNR: -104.522\tR: -3.953\tLR: -100.000\n","Epi: 209\tSteps: 54\tQV: -3.222e+01\tQL: 2.130e+01\tPR: 1.293\tNR: -105.491\tR: -4.199\tLR: -100.000\n","Epi: 210\tSteps: 71\tQV: -3.223e+01\tQL: 2.797e+01\tPR: 2.656\tNR: -105.363\tR: -2.707\tLR: -100.000\n","Epi: 211\tSteps: 73\tQV: -3.218e+01\tQL: 2.373e+01\tPR: 4.386\tNR: -105.948\tR: -1.562\tLR: -100.000\n","Epi: 212\tSteps: 70\tQV: -3.225e+01\tQL: 2.284e+01\tPR: 4.013\tNR: -105.494\tR: -1.481\tLR: -100.000\n","Epi: 213\tSteps: 46\tQV: -3.223e+01\tQL: 2.250e+01\tPR: 1.245\tNR: -104.922\tR: -3.677\tLR: -100.000\n","Epi: 214\tSteps: 47\tQV: -3.216e+01\tQL: 1.895e+01\tPR: 0.214\tNR: -104.635\tR: -4.421\tLR: -100.000\n","Epi: 215\tSteps: 59\tQV: -3.228e+01\tQL: 2.517e+01\tPR: 2.777\tNR: -106.360\tR: -3.583\tLR: -100.000\n","Epi: 216\tSteps: 91\tQV: -3.226e+01\tQL: 2.301e+01\tPR: 6.165\tNR: -105.401\tR: 0.764\tLR: -100.000\n","Epi: 217\tSteps: 68\tQV: -3.225e+01\tQL: 1.982e+01\tPR: 3.365\tNR: -106.374\tR: -3.009\tLR: -100.000\n","Epi: 218\tSteps: 101\tQV: -3.230e+01\tQL: 2.281e+01\tPR: 2.681\tNR: -108.692\tR: -6.012\tLR: -100.000\n","Epi: 219\tSteps: 64\tQV: -3.216e+01\tQL: 1.789e+01\tPR: 2.211\tNR: -106.277\tR: -4.067\tLR: -100.000\n","Epi: 220\tSteps: 115\tQV: -3.247e+01\tQL: 2.998e+01\tPR: 6.393\tNR: -105.505\tR: 0.888\tLR: -100.000\n","Epi: 221\tSteps: 80\tQV: -3.224e+01\tQL: 1.794e+01\tPR: 5.685\tNR: -107.111\tR: -1.426\tLR: -100.000\n","Epi: 222\tSteps: 66\tQV: -3.231e+01\tQL: 1.955e+01\tPR: 4.125\tNR: -107.446\tR: -3.321\tLR: -100.000\n","Epi: 223\tSteps: 58\tQV: -3.241e+01\tQL: 2.399e+01\tPR: 1.913\tNR: -106.292\tR: -4.379\tLR: -100.000\n","Epi: 224\tSteps: 71\tQV: -3.236e+01\tQL: 2.021e+01\tPR: 4.222\tNR: -107.002\tR: -2.780\tLR: -100.000\n","Epi: 225\tSteps: 75\tQV: -3.244e+01\tQL: 2.249e+01\tPR: 4.628\tNR: -106.491\tR: -1.864\tLR: -100.000\n","Epi: 226\tSteps: 71\tQV: -3.240e+01\tQL: 2.043e+01\tPR: 3.500\tNR: -105.909\tR: -2.409\tLR: -100.000\n","Epi: 227\tSteps: 76\tQV: -3.244e+01\tQL: 2.088e+01\tPR: 5.324\tNR: -105.121\tR: 0.203\tLR: -100.000\n","Epi: 228\tSteps: 53\tQV: -3.241e+01\tQL: 2.122e+01\tPR: 2.230\tNR: -106.565\tR: -4.335\tLR: -100.000\n","Epi: 229\tSteps: 74\tQV: -3.243e+01\tQL: 2.427e+01\tPR: 3.016\tNR: -104.976\tR: -1.959\tLR: -100.000\n","Epi: 230\tSteps: 80\tQV: -3.245e+01\tQL: 2.012e+01\tPR: 5.369\tNR: -105.072\tR: 0.297\tLR: -100.000\n","Epi: 231\tSteps: 84\tQV: -3.254e+01\tQL: 2.560e+01\tPR: 4.008\tNR: -105.650\tR: -1.642\tLR: -100.000\n","Epi: 232\tSteps: 68\tQV: -3.250e+01\tQL: 2.352e+01\tPR: 4.038\tNR: -106.871\tR: -2.833\tLR: -100.000\n","Epi: 233\tSteps: 42\tQV: -3.259e+01\tQL: 2.760e+01\tPR: 0.066\tNR: -104.728\tR: -4.662\tLR: -100.000\n","Epi: 234\tSteps: 65\tQV: -3.251e+01\tQL: 2.200e+01\tPR: 4.476\tNR: -104.729\tR: -0.253\tLR: -100.000\n","Epi: 235\tSteps: 84\tQV: -3.247e+01\tQL: 1.889e+01\tPR: 3.552\tNR: -108.127\tR: -4.575\tLR: -100.000\n","Epi: 236\tSteps: 72\tQV: -3.254e+01\tQL: 2.261e+01\tPR: 4.031\tNR: -107.082\tR: -3.051\tLR: -100.000\n","Epi: 237\tSteps: 73\tQV: -3.255e+01\tQL: 2.275e+01\tPR: 4.384\tNR: -105.429\tR: -1.045\tLR: -100.000\n","Epi: 238\tSteps: 165\tQV: -3.253e+01\tQL: 2.220e+01\tPR: 0.246\tNR: -139.635\tR: -39.388\tLR: -100.000\n","Epi: 239\tSteps: 68\tQV: -3.255e+01\tQL: 2.031e+01\tPR: 4.640\tNR: -105.792\tR: -1.152\tLR: -100.000\n","Epi: 240\tSteps: 96\tQV: -3.264e+01\tQL: 2.584e+01\tPR: 5.053\tNR: -106.771\tR: -1.718\tLR: -100.000\n","Epi: 241\tSteps: 59\tQV: -3.249e+01\tQL: 1.676e+01\tPR: 2.951\tNR: -106.640\tR: -3.690\tLR: -100.000\n","Epi: 242\tSteps: 150\tQV: -3.268e+01\tQL: 2.385e+01\tPR: 0.373\tNR: -135.371\tR: -34.998\tLR: -100.000\n","Epi: 243\tSteps: 66\tQV: -3.262e+01\tQL: 2.075e+01\tPR: 3.582\tNR: -107.699\tR: -4.117\tLR: -100.000\n","Epi: 244\tSteps: 102\tQV: -3.269e+01\tQL: 2.619e+01\tPR: 5.593\tNR: -106.541\tR: -0.948\tLR: -100.000\n","Epi: 245\tSteps: 67\tQV: -3.260e+01\tQL: 1.968e+01\tPR: 3.864\tNR: -106.270\tR: -2.406\tLR: -100.000\n","Epi: 246\tSteps: 73\tQV: -3.267e+01\tQL: 2.163e+01\tPR: 5.626\tNR: -105.970\tR: -0.344\tLR: -100.000\n","Epi: 247\tSteps: 59\tQV: -3.266e+01\tQL: 2.171e+01\tPR: 3.945\tNR: -106.944\tR: -2.999\tLR: -100.000\n","Epi: 248\tSteps: 61\tQV: -3.276e+01\tQL: 2.497e+01\tPR: 2.944\tNR: -105.217\tR: -2.273\tLR: -100.000\n","Epi: 249\tSteps: 66\tQV: -3.285e+01\tQL: 2.550e+01\tPR: 2.533\tNR: -105.743\tR: -3.210\tLR: -100.000\n","Epi: 250\tSteps: 88\tQV: -3.266e+01\tQL: 2.226e+01\tPR: 6.007\tNR: -105.469\tR: 0.539\tLR: -100.000\n","Epi: 251\tSteps: 67\tQV: -3.276e+01\tQL: 2.515e+01\tPR: 2.459\tNR: -104.289\tR: -1.831\tLR: -100.000\n","Epi: 252\tSteps: 55\tQV: -3.271e+01\tQL: 2.045e+01\tPR: 3.006\tNR: -104.977\tR: -1.971\tLR: -100.000\n","Epi: 253\tSteps: 78\tQV: -3.285e+01\tQL: 2.492e+01\tPR: 5.377\tNR: -106.522\tR: -1.145\tLR: -100.000\n","Epi: 254\tSteps: 65\tQV: -3.273e+01\tQL: 2.224e+01\tPR: 4.812\tNR: -104.861\tR: -0.049\tLR: -100.000\n","Epi: 255\tSteps: 50\tQV: -3.284e+01\tQL: 2.367e+01\tPR: 2.333\tNR: -105.819\tR: -3.486\tLR: -100.000\n","Epi: 256\tSteps: 67\tQV: -3.291e+01\tQL: 2.979e+01\tPR: 4.680\tNR: -105.638\tR: -0.958\tLR: -100.000\n","Epi: 257\tSteps: 65\tQV: -3.278e+01\tQL: 2.518e+01\tPR: 3.137\tNR: -106.891\tR: -3.753\tLR: -100.000\n","Epi: 258\tSteps: 57\tQV: -3.280e+01\tQL: 2.267e+01\tPR: 3.322\tNR: -106.751\tR: -3.429\tLR: -100.000\n","Epi: 259\tSteps: 52\tQV: -3.290e+01\tQL: 2.649e+01\tPR: 3.292\tNR: -105.759\tR: -2.468\tLR: -100.000\n","Epi: 260\tSteps: 75\tQV: -3.281e+01\tQL: 2.146e+01\tPR: 6.027\tNR: -105.464\tR: 0.563\tLR: -100.000\n","Epi: 261\tSteps: 74\tQV: -3.274e+01\tQL: 1.579e+01\tPR: 4.923\tNR: -106.043\tR: -1.120\tLR: -100.000\n","Epi: 262\tSteps: 66\tQV: -3.282e+01\tQL: 2.170e+01\tPR: 4.554\tNR: -105.842\tR: -1.288\tLR: -100.000\n","Epi: 263\tSteps: 74\tQV: -3.288e+01\tQL: 2.200e+01\tPR: 6.236\tNR: -106.781\tR: -0.545\tLR: -100.000\n","Epi: 264\tSteps: 82\tQV: -3.288e+01\tQL: 2.142e+01\tPR: 4.319\tNR: -105.765\tR: -1.447\tLR: -100.000\n","Epi: 265\tSteps: 63\tQV: -3.290e+01\tQL: 2.513e+01\tPR: 4.696\tNR: -108.390\tR: -3.695\tLR: -100.000\n","Epi: 266\tSteps: 75\tQV: -3.288e+01\tQL: 2.555e+01\tPR: 7.109\tNR: -107.482\tR: -0.372\tLR: -100.000\n","Epi: 267\tSteps: 75\tQV: -3.297e+01\tQL: 2.345e+01\tPR: 5.564\tNR: -106.102\tR: -0.537\tLR: -100.000\n","Epi: 268\tSteps: 79\tQV: -3.307e+01\tQL: 3.077e+01\tPR: 6.656\tNR: -108.123\tR: -1.467\tLR: -100.000\n","Epi: 269\tSteps: 65\tQV: -3.294e+01\tQL: 1.986e+01\tPR: 5.260\tNR: -106.271\tR: -1.011\tLR: -100.000\n","Epi: 270\tSteps: 56\tQV: -3.292e+01\tQL: 1.759e+01\tPR: 3.355\tNR: -104.809\tR: -1.454\tLR: -100.000\n","Epi: 271\tSteps: 53\tQV: -3.305e+01\tQL: 2.949e+01\tPR: 3.482\tNR: -105.779\tR: -2.297\tLR: -100.000\n","Epi: 272\tSteps: 64\tQV: -3.297e+01\tQL: 2.571e+01\tPR: 4.460\tNR: -106.580\tR: -2.120\tLR: -100.000\n","Epi: 273\tSteps: 78\tQV: -3.291e+01\tQL: 1.986e+01\tPR: 5.908\tNR: -107.330\tR: -1.423\tLR: -100.000\n","Epi: 274\tSteps: 65\tQV: -3.296e+01\tQL: 2.060e+01\tPR: 5.323\tNR: -107.345\tR: -2.022\tLR: -100.000\n","Epi: 275\tSteps: 85\tQV: -3.301e+01\tQL: 2.210e+01\tPR: 7.536\tNR: -106.860\tR: 0.676\tLR: -100.000\n","Epi: 276\tSteps: 66\tQV: -3.314e+01\tQL: 2.685e+01\tPR: 3.529\tNR: -106.543\tR: -3.014\tLR: -100.000\n","Epi: 277\tSteps: 74\tQV: -3.305e+01\tQL: 2.409e+01\tPR: 4.750\tNR: -106.879\tR: -2.130\tLR: -100.000\n","Epi: 278\tSteps: 53\tQV: -3.310e+01\tQL: 2.269e+01\tPR: 4.792\tNR: -106.318\tR: -1.527\tLR: -100.000\n","Epi: 279\tSteps: 79\tQV: -3.307e+01\tQL: 2.238e+01\tPR: 5.556\tNR: -107.708\tR: -2.152\tLR: -100.000\n","Epi: 280\tSteps: 63\tQV: -3.310e+01\tQL: 2.496e+01\tPR: 4.399\tNR: -106.150\tR: -1.751\tLR: -100.000\n","Epi: 281\tSteps: 69\tQV: -3.302e+01\tQL: 1.884e+01\tPR: 4.706\tNR: -106.862\tR: -2.157\tLR: -100.000\n","Epi: 282\tSteps: 70\tQV: -3.305e+01\tQL: 2.379e+01\tPR: 2.862\tNR: -108.467\tR: -5.605\tLR: -100.000\n","Epi: 283\tSteps: 98\tQV: -3.316e+01\tQL: 2.548e+01\tPR: 0.000\tNR: -121.130\tR: -21.130\tLR: -100.000\n","Epi: 284\tSteps: 135\tQV: -3.318e+01\tQL: 2.496e+01\tPR: 2.076\tNR: -132.594\tR: -30.518\tLR: -100.000\n","Epi: 285\tSteps: 91\tQV: -3.318e+01\tQL: 2.668e+01\tPR: 3.423\tNR: -106.789\tR: -3.365\tLR: -100.000\n","Epi: 286\tSteps: 80\tQV: -3.314e+01\tQL: 2.188e+01\tPR: 4.511\tNR: -106.679\tR: -2.168\tLR: -100.000\n","Epi: 287\tSteps: 56\tQV: -3.319e+01\tQL: 2.287e+01\tPR: 0.459\tNR: -105.018\tR: -4.559\tLR: -100.000\n","Epi: 288\tSteps: 43\tQV: -3.321e+01\tQL: 2.385e+01\tPR: 0.028\tNR: -105.335\tR: -5.306\tLR: -100.000\n","Epi: 289\tSteps: 126\tQV: -3.314e+01\tQL: 2.051e+01\tPR: 1.797\tNR: -116.696\tR: -14.899\tLR: -100.000\n","Epi: 290\tSteps: 46\tQV: -3.320e+01\tQL: 2.470e+01\tPR: 0.351\tNR: -105.847\tR: -5.497\tLR: -100.000\n","Epi: 291\tSteps: 56\tQV: -3.320e+01\tQL: 2.425e+01\tPR: 0.539\tNR: -104.321\tR: -3.782\tLR: -100.000\n","Epi: 292\tSteps: 64\tQV: -3.331e+01\tQL: 2.564e+01\tPR: 0.871\tNR: -103.109\tR: -2.238\tLR: -100.000\n","Epi: 293\tSteps: 42\tQV: -3.320e+01\tQL: 2.564e+01\tPR: 0.078\tNR: -105.895\tR: -5.817\tLR: -100.000\n","Epi: 294\tSteps: 52\tQV: -3.334e+01\tQL: 2.616e+01\tPR: 0.663\tNR: -106.337\tR: -5.674\tLR: -100.000\n","Epi: 295\tSteps: 58\tQV: -3.323e+01\tQL: 2.410e+01\tPR: 0.512\tNR: -105.747\tR: -5.234\tLR: -100.000\n","Epi: 296\tSteps: 60\tQV: -3.333e+01\tQL: 2.502e+01\tPR: 0.544\tNR: -104.041\tR: -3.497\tLR: -100.000\n","Epi: 297\tSteps: 84\tQV: -3.329e+01\tQL: 2.448e+01\tPR: 2.177\tNR: -108.769\tR: -6.592\tLR: -100.000\n","Epi: 298\tSteps: 71\tQV: -3.329e+01\tQL: 2.185e+01\tPR: 2.946\tNR: -107.454\tR: -4.508\tLR: -100.000\n","Epi: 299\tSteps: 57\tQV: -3.343e+01\tQL: 2.927e+01\tPR: 2.203\tNR: -106.590\tR: -4.387\tLR: -100.000\n","Epi: 300\tSteps: 63\tQV: -3.342e+01\tQL: 2.965e+01\tPR: 1.585\tNR: -108.506\tR: -6.922\tLR: -100.000\n","Epi: 301\tSteps: 59\tQV: -3.333e+01\tQL: 2.389e+01\tPR: 1.435\tNR: -105.568\tR: -4.132\tLR: -100.000\n","Epi: 302\tSteps: 59\tQV: -3.332e+01\tQL: 2.428e+01\tPR: 1.780\tNR: -105.713\tR: -3.933\tLR: -100.000\n","Epi: 303\tSteps: 63\tQV: -3.337e+01\tQL: 2.219e+01\tPR: 1.870\tNR: -110.754\tR: -8.884\tLR: -100.000\n","Epi: 304\tSteps: 51\tQV: -3.335e+01\tQL: 2.303e+01\tPR: 0.659\tNR: -106.631\tR: -5.972\tLR: -100.000\n","Epi: 305\tSteps: 40\tQV: -3.340e+01\tQL: 2.239e+01\tPR: 0.000\tNR: -107.127\tR: -7.127\tLR: -100.000\n","Epi: 306\tSteps: 91\tQV: -3.345e+01\tQL: 2.590e+01\tPR: 1.374\tNR: -108.771\tR: -7.396\tLR: -100.000\n","Epi: 307\tSteps: 72\tQV: -3.342e+01\tQL: 2.420e+01\tPR: 2.734\tNR: -104.793\tR: -2.058\tLR: -100.000\n","Epi: 308\tSteps: 56\tQV: -3.333e+01\tQL: 1.846e+01\tPR: 0.804\tNR: -105.746\tR: -4.942\tLR: -100.000\n","Epi: 309\tSteps: 46\tQV: -3.353e+01\tQL: 2.872e+01\tPR: 0.120\tNR: -109.707\tR: -9.588\tLR: -100.000\n","Epi: 310\tSteps: 62\tQV: -3.339e+01\tQL: 2.300e+01\tPR: 1.623\tNR: -105.802\tR: -4.179\tLR: -100.000\n","Epi: 311\tSteps: 97\tQV: -3.345e+01\tQL: 2.193e+01\tPR: 3.136\tNR: -108.691\tR: -5.555\tLR: -100.000\n","Epi: 312\tSteps: 56\tQV: -3.344e+01\tQL: 2.388e+01\tPR: 0.686\tNR: -106.338\tR: -5.652\tLR: -100.000\n","Epi: 313\tSteps: 90\tQV: -3.350e+01\tQL: 2.413e+01\tPR: 2.560\tNR: -107.935\tR: -5.375\tLR: -100.000\n","Epi: 314\tSteps: 60\tQV: -3.348e+01\tQL: 2.260e+01\tPR: 0.699\tNR: -106.489\tR: -5.791\tLR: -100.000\n","Epi: 315\tSteps: 122\tQV: -3.348e+01\tQL: 2.257e+01\tPR: 1.394\tNR: -109.967\tR: -8.573\tLR: -100.000\n","Epi: 316\tSteps: 156\tQV: -3.364e+01\tQL: 2.714e+01\tPR: 3.288\tNR: -110.798\tR: -7.510\tLR: -100.000\n","Epi: 317\tSteps: 56\tQV: -3.377e+01\tQL: 3.470e+01\tPR: 0.593\tNR: -109.620\tR: -9.027\tLR: -100.000\n","Epi: 318\tSteps: 40\tQV: -3.351e+01\tQL: 2.380e+01\tPR: 0.000\tNR: -108.515\tR: -8.515\tLR: -100.000\n","Epi: 319\tSteps: 39\tQV: -3.347e+01\tQL: 1.620e+01\tPR: 0.004\tNR: -107.343\tR: -7.339\tLR: -100.000\n","Epi: 320\tSteps: 82\tQV: -3.362e+01\tQL: 2.667e+01\tPR: 4.894\tNR: -104.007\tR: 0.887\tLR: -100.000\n","Epi: 321\tSteps: 72\tQV: -3.355e+01\tQL: 2.422e+01\tPR: 1.853\tNR: -103.630\tR: -1.776\tLR: -100.000\n","Epi: 322\tSteps: 80\tQV: -3.355e+01\tQL: 2.271e+01\tPR: 3.716\tNR: -104.267\tR: -0.551\tLR: -100.000\n","Epi: 323\tSteps: 74\tQV: -3.364e+01\tQL: 2.417e+01\tPR: 3.887\tNR: -104.676\tR: -0.789\tLR: -100.000\n","Epi: 324\tSteps: 56\tQV: -3.355e+01\tQL: 2.386e+01\tPR: 2.146\tNR: -104.573\tR: -2.427\tLR: -100.000\n","Epi: 325\tSteps: 70\tQV: -3.370e+01\tQL: 2.712e+01\tPR: 1.916\tNR: -108.902\tR: -6.985\tLR: -100.000\n","Epi: 326\tSteps: 51\tQV: -3.363e+01\tQL: 2.687e+01\tPR: 3.007\tNR: -104.133\tR: -1.126\tLR: -100.000\n","Epi: 327\tSteps: 50\tQV: -3.364e+01\tQL: 2.438e+01\tPR: 2.448\tNR: -104.960\tR: -2.511\tLR: -100.000\n","Epi: 328\tSteps: 46\tQV: -3.361e+01\tQL: 1.554e+01\tPR: 2.153\tNR: -104.459\tR: -2.305\tLR: -100.000\n","Epi: 329\tSteps: 53\tQV: -3.353e+01\tQL: 2.073e+01\tPR: 2.273\tNR: -104.963\tR: -2.690\tLR: -100.000\n","Epi: 330\tSteps: 54\tQV: -3.377e+01\tQL: 2.753e+01\tPR: 3.291\tNR: -103.887\tR: -0.597\tLR: -100.000\n","Epi: 331\tSteps: 52\tQV: -3.362e+01\tQL: 2.294e+01\tPR: 2.450\tNR: -103.963\tR: -1.513\tLR: -100.000\n","Epi: 332\tSteps: 56\tQV: -3.374e+01\tQL: 2.824e+01\tPR: 2.515\tNR: -107.910\tR: -5.396\tLR: -100.000\n","Epi: 333\tSteps: 53\tQV: -3.371e+01\tQL: 2.790e+01\tPR: 2.520\tNR: -104.694\tR: -2.174\tLR: -100.000\n","Epi: 334\tSteps: 57\tQV: -3.368e+01\tQL: 2.300e+01\tPR: 1.372\tNR: -111.243\tR: -9.872\tLR: -100.000\n","Epi: 335\tSteps: 45\tQV: -3.382e+01\tQL: 2.579e+01\tPR: 1.974\tNR: -104.423\tR: -2.449\tLR: -100.000\n","Epi: 336\tSteps: 46\tQV: -3.367e+01\tQL: 1.971e+01\tPR: 1.973\tNR: -104.159\tR: -2.186\tLR: -100.000\n","Epi: 337\tSteps: 53\tQV: -3.374e+01\tQL: 2.623e+01\tPR: 2.218\tNR: -105.995\tR: -3.776\tLR: -100.000\n","Epi: 338\tSteps: 53\tQV: -3.379e+01\tQL: 2.464e+01\tPR: 2.396\tNR: -104.289\tR: -1.894\tLR: -100.000\n","Epi: 339\tSteps: 55\tQV: -3.382e+01\tQL: 2.876e+01\tPR: 2.960\tNR: -105.156\tR: -2.196\tLR: -100.000\n","Epi: 340\tSteps: 61\tQV: -3.383e+01\tQL: 2.581e+01\tPR: 3.146\tNR: -105.393\tR: -2.247\tLR: -100.000\n","Epi: 341\tSteps: 49\tQV: -3.370e+01\tQL: 2.269e+01\tPR: 2.464\tNR: -103.656\tR: -1.192\tLR: -100.000\n","Epi: 342\tSteps: 42\tQV: -3.376e+01\tQL: 2.439e+01\tPR: 0.845\tNR: -104.127\tR: -3.282\tLR: -100.000\n","Epi: 343\tSteps: 47\tQV: -3.375e+01\tQL: 2.703e+01\tPR: 1.993\tNR: -104.713\tR: -2.720\tLR: -100.000\n","Epi: 344\tSteps: 49\tQV: -3.376e+01\tQL: 2.642e+01\tPR: 2.237\tNR: -104.046\tR: -1.809\tLR: -100.000\n","Epi: 345\tSteps: 43\tQV: -3.387e+01\tQL: 2.706e+01\tPR: 1.268\tNR: -104.400\tR: -3.133\tLR: -100.000\n","Epi: 346\tSteps: 44\tQV: -3.387e+01\tQL: 2.793e+01\tPR: 1.561\tNR: -104.729\tR: -3.168\tLR: -100.000\n","Epi: 347\tSteps: 48\tQV: -3.373e+01\tQL: 1.881e+01\tPR: 2.274\tNR: -104.173\tR: -1.899\tLR: -100.000\n","Epi: 348\tSteps: 46\tQV: -3.380e+01\tQL: 2.514e+01\tPR: 2.191\tNR: -104.443\tR: -2.252\tLR: -100.000\n","Epi: 349\tSteps: 45\tQV: -3.380e+01\tQL: 1.802e+01\tPR: 1.814\tNR: -104.419\tR: -2.605\tLR: -100.000\n","Epi: 350\tSteps: 45\tQV: -3.379e+01\tQL: 1.928e+01\tPR: 0.620\tNR: -104.140\tR: -3.520\tLR: -100.000\n","Epi: 351\tSteps: 48\tQV: -3.391e+01\tQL: 3.059e+01\tPR: 2.697\tNR: -104.795\tR: -2.098\tLR: -100.000\n","Epi: 352\tSteps: 43\tQV: -3.379e+01\tQL: 2.427e+01\tPR: 1.738\tNR: -104.597\tR: -2.859\tLR: -100.000\n","Epi: 353\tSteps: 46\tQV: -3.385e+01\tQL: 2.611e+01\tPR: 2.689\tNR: -107.671\tR: -4.982\tLR: -100.000\n","Epi: 354\tSteps: 46\tQV: -3.398e+01\tQL: 2.589e+01\tPR: 2.054\tNR: -104.124\tR: -2.070\tLR: -100.000\n","Epi: 355\tSteps: 46\tQV: -3.395e+01\tQL: 3.143e+01\tPR: 3.083\tNR: -104.728\tR: -1.645\tLR: -100.000\n","Epi: 356\tSteps: 45\tQV: -3.394e+01\tQL: 2.744e+01\tPR: 2.024\tNR: -104.503\tR: -2.479\tLR: -100.000\n","Epi: 357\tSteps: 48\tQV: -3.401e+01\tQL: 2.889e+01\tPR: 2.414\tNR: -104.665\tR: -2.251\tLR: -100.000\n","Epi: 358\tSteps: 47\tQV: -3.388e+01\tQL: 2.304e+01\tPR: 2.319\tNR: -104.890\tR: -2.571\tLR: -100.000\n","Epi: 359\tSteps: 43\tQV: -3.392e+01\tQL: 2.409e+01\tPR: 1.386\tNR: -104.692\tR: -3.305\tLR: -100.000\n","Epi: 360\tSteps: 48\tQV: -3.400e+01\tQL: 3.008e+01\tPR: 2.216\tNR: -106.170\tR: -3.954\tLR: -100.000\n","Epi: 361\tSteps: 47\tQV: -3.398e+01\tQL: 2.132e+01\tPR: 0.943\tNR: -109.333\tR: -8.390\tLR: -100.000\n","Epi: 362\tSteps: 48\tQV: -3.401e+01\tQL: 2.923e+01\tPR: 2.424\tNR: -104.945\tR: -2.521\tLR: -100.000\n","Epi: 363\tSteps: 45\tQV: -3.390e+01\tQL: 2.087e+01\tPR: 1.625\tNR: -104.706\tR: -3.080\tLR: -100.000\n","Epi: 364\tSteps: 46\tQV: -3.393e+01\tQL: 2.505e+01\tPR: 1.862\tNR: -104.912\tR: -3.051\tLR: -100.000\n","Epi: 365\tSteps: 61\tQV: -3.407e+01\tQL: 2.719e+01\tPR: 2.250\tNR: -108.875\tR: -6.625\tLR: -100.000\n","Epi: 366\tSteps: 52\tQV: -3.413e+01\tQL: 3.217e+01\tPR: 3.469\tNR: -105.548\tR: -2.079\tLR: -100.000\n","Epi: 367\tSteps: 41\tQV: -3.409e+01\tQL: 2.376e+01\tPR: 0.216\tNR: -106.735\tR: -6.520\tLR: -100.000\n","Epi: 368\tSteps: 47\tQV: -3.412e+01\tQL: 2.647e+01\tPR: 1.739\tNR: -104.579\tR: -2.841\tLR: -100.000\n","Epi: 369\tSteps: 49\tQV: -3.411e+01\tQL: 3.553e+01\tPR: 2.455\tNR: -103.848\tR: -1.393\tLR: -100.000\n","Epi: 370\tSteps: 46\tQV: -3.398e+01\tQL: 1.991e+01\tPR: 0.160\tNR: -107.180\tR: -7.020\tLR: -100.000\n","Epi: 371\tSteps: 49\tQV: -3.400e+01\tQL: 2.185e+01\tPR: 2.162\tNR: -105.015\tR: -2.853\tLR: -100.000\n","Epi: 372\tSteps: 51\tQV: -3.416e+01\tQL: 2.890e+01\tPR: 2.283\tNR: -107.591\tR: -5.308\tLR: -100.000\n","Epi: 373\tSteps: 43\tQV: -3.411e+01\tQL: 2.857e+01\tPR: 0.311\tNR: -110.158\tR: -9.847\tLR: -100.000\n","Epi: 374\tSteps: 49\tQV: -3.410e+01\tQL: 2.465e+01\tPR: 2.646\tNR: -105.064\tR: -2.417\tLR: -100.000\n","Epi: 375\tSteps: 68\tQV: -3.410e+01\tQL: 2.450e+01\tPR: 4.660\tNR: -105.317\tR: -0.656\tLR: -100.000\n","Epi: 376\tSteps: 47\tQV: -3.419e+01\tQL: 2.590e+01\tPR: 0.704\tNR: -104.311\tR: -3.607\tLR: -100.000\n","Epi: 377\tSteps: 81\tQV: -3.401e+01\tQL: 2.332e+01\tPR: 4.043\tNR: -105.872\tR: -1.828\tLR: -100.000\n","Epi: 378\tSteps: 58\tQV: -3.431e+01\tQL: 3.503e+01\tPR: 2.907\tNR: -104.768\tR: -1.861\tLR: -100.000\n","Epi: 379\tSteps: 89\tQV: -3.420e+01\tQL: 2.802e+01\tPR: 2.482\tNR: -106.213\tR: -3.731\tLR: -100.000\n","Epi: 380\tSteps: 41\tQV: -3.424e+01\tQL: 2.252e+01\tPR: 0.128\tNR: -104.870\tR: -4.743\tLR: -100.000\n","Epi: 381\tSteps: 39\tQV: -3.417e+01\tQL: 2.574e+01\tPR: 0.000\tNR: -108.251\tR: -8.251\tLR: -100.000\n","Epi: 382\tSteps: 38\tQV: -3.423e+01\tQL: 2.745e+01\tPR: 0.000\tNR: -109.517\tR: -9.517\tLR: -100.000\n","Epi: 383\tSteps: 46\tQV: -3.400e+01\tQL: 1.572e+01\tPR: 0.643\tNR: -104.683\tR: -4.039\tLR: -100.000\n","Epi: 384\tSteps: 60\tQV: -3.427e+01\tQL: 2.565e+01\tPR: 3.325\tNR: -103.068\tR: 0.257\tLR: -100.000\n","Epi: 385\tSteps: 68\tQV: -3.427e+01\tQL: 2.736e+01\tPR: 1.806\tNR: -104.605\tR: -2.799\tLR: -100.000\n","Epi: 386\tSteps: 71\tQV: -3.418e+01\tQL: 2.881e+01\tPR: 0.705\tNR: -121.733\tR: -21.028\tLR: -100.000\n","Epi: 387\tSteps: 49\tQV: -3.428e+01\tQL: 2.369e+01\tPR: 2.089\tNR: -103.884\tR: -1.795\tLR: -100.000\n","Epi: 388\tSteps: 36\tQV: -3.414e+01\tQL: 1.838e+01\tPR: 0.000\tNR: -110.867\tR: -10.867\tLR: -100.000\n","Epi: 389\tSteps: 77\tQV: -3.425e+01\tQL: 2.186e+01\tPR: 3.191\tNR: -108.180\tR: -4.989\tLR: -100.000\n","Epi: 390\tSteps: 58\tQV: -3.434e+01\tQL: 2.882e+01\tPR: 0.451\tNR: -105.476\tR: -5.025\tLR: -100.000\n","Epi: 391\tSteps: 40\tQV: -3.431e+01\tQL: 2.419e+01\tPR: 0.022\tNR: -105.852\tR: -5.829\tLR: -100.000\n","Epi: 392\tSteps: 71\tQV: -3.435e+01\tQL: 2.430e+01\tPR: 4.185\tNR: -104.175\tR: 0.010\tLR: -100.000\n","Epi: 393\tSteps: 82\tQV: -3.423e+01\tQL: 2.066e+01\tPR: 1.600\tNR: -114.103\tR: -12.503\tLR: -100.000\n","Epi: 394\tSteps: 57\tQV: -3.432e+01\tQL: 2.292e+01\tPR: 1.242\tNR: -108.864\tR: -7.622\tLR: -100.000\n","Epi: 395\tSteps: 87\tQV: -3.439e+01\tQL: 2.732e+01\tPR: 0.915\tNR: -122.495\tR: -21.580\tLR: -100.000\n","Epi: 396\tSteps: 63\tQV: -3.444e+01\tQL: 2.998e+01\tPR: 3.935\tNR: -103.372\tR: 0.563\tLR: -100.000\n","Epi: 397\tSteps: 190\tQV: -3.440e+01\tQL: 2.589e+01\tPR: 6.316\tNR: -120.789\tR: -14.473\tLR: -100.000\n","Epi: 398\tSteps: 74\tQV: -3.442e+01\tQL: 2.516e+01\tPR: 0.192\tNR: -118.930\tR: -18.737\tLR: -100.000\n","Epi: 399\tSteps: 60\tQV: -3.441e+01\tQL: 2.838e+01\tPR: 1.181\tNR: -105.013\tR: -3.832\tLR: -100.000\n","Epi: 400\tSteps: 77\tQV: -3.444e+01\tQL: 2.545e+01\tPR: 5.072\tNR: -104.545\tR: 0.528\tLR: -100.000\n","Epi: 401\tSteps: 92\tQV: -3.443e+01\tQL: 2.819e+01\tPR: 6.039\tNR: -104.442\tR: 1.596\tLR: -100.000\n","Epi: 402\tSteps: 59\tQV: -3.459e+01\tQL: 3.172e+01\tPR: 1.726\tNR: -109.540\tR: -7.814\tLR: -100.000\n","Epi: 403\tSteps: 62\tQV: -3.470e+01\tQL: 3.725e+01\tPR: 2.183\tNR: -105.181\tR: -2.997\tLR: -100.000\n","Epi: 404\tSteps: 104\tQV: -3.453e+01\tQL: 2.758e+01\tPR: 7.000\tNR: -105.637\tR: 1.364\tLR: -100.000\n","Epi: 405\tSteps: 87\tQV: -3.454e+01\tQL: 2.886e+01\tPR: 4.673\tNR: -105.938\tR: -1.265\tLR: -100.000\n","Epi: 406\tSteps: 93\tQV: -3.459e+01\tQL: 2.781e+01\tPR: 0.203\tNR: -125.534\tR: -25.331\tLR: -100.000\n","Epi: 407\tSteps: 89\tQV: -3.461e+01\tQL: 2.526e+01\tPR: 8.036\tNR: -104.780\tR: 3.257\tLR: -100.000\n","Epi: 408\tSteps: 65\tQV: -3.437e+01\tQL: 1.655e+01\tPR: 5.349\tNR: -102.785\tR: 2.564\tLR: -100.000\n","Epi: 409\tSteps: 111\tQV: -3.455e+01\tQL: 2.184e+01\tPR: 9.625\tNR: -104.450\tR: 5.176\tLR: -100.000\n","Epi: 410\tSteps: 96\tQV: -3.475e+01\tQL: 3.060e+01\tPR: 1.078\tNR: -114.970\tR: -13.893\tLR: -100.000\n","Epi: 411\tSteps: 61\tQV: -3.458e+01\tQL: 2.652e+01\tPR: 4.963\tNR: -105.202\tR: -0.239\tLR: -100.000\n","Epi: 412\tSteps: 72\tQV: -3.475e+01\tQL: 3.402e+01\tPR: 5.813\tNR: -103.944\tR: 1.869\tLR: -100.000\n","Epi: 413\tSteps: 60\tQV: -3.473e+01\tQL: 2.781e+01\tPR: 2.873\tNR: -103.475\tR: -0.602\tLR: -100.000\n","Epi: 414\tSteps: 71\tQV: -3.475e+01\tQL: 3.054e+01\tPR: 3.551\tNR: -104.066\tR: -0.515\tLR: -100.000\n","Epi: 415\tSteps: 58\tQV: -3.481e+01\tQL: 2.805e+01\tPR: 1.802\tNR: -115.698\tR: -13.896\tLR: -100.000\n","Epi: 416\tSteps: 69\tQV: -3.472e+01\tQL: 2.503e+01\tPR: 6.137\tNR: -103.943\tR: 2.194\tLR: -100.000\n","Epi: 417\tSteps: 89\tQV: -3.469e+01\tQL: 2.412e+01\tPR: 4.178\tNR: -105.381\tR: -1.203\tLR: -100.000\n","Epi: 418\tSteps: 101\tQV: -3.472e+01\tQL: 2.513e+01\tPR: 10.817\tNR: -103.116\tR: 7.701\tLR: -100.000\n","Epi: 419\tSteps: 78\tQV: -3.467e+01\tQL: 2.210e+01\tPR: 8.699\tNR: -103.725\tR: 4.974\tLR: -100.000\n","Epi: 420\tSteps: 93\tQV: -3.473e+01\tQL: 2.466e+01\tPR: 5.562\tNR: -106.643\tR: -1.081\tLR: -100.000\n","Epi: 421\tSteps: 50\tQV: -3.470e+01\tQL: 2.304e+01\tPR: 2.061\tNR: -104.869\tR: -2.809\tLR: -100.000\n","Epi: 422\tSteps: 134\tQV: -3.487e+01\tQL: 2.935e+01\tPR: 1.894\tNR: -126.447\tR: -24.553\tLR: -100.000\n","Epi: 423\tSteps: 66\tQV: -3.482e+01\tQL: 2.523e+01\tPR: 0.000\tNR: -116.649\tR: -16.649\tLR: -100.000\n","Epi: 424\tSteps: 83\tQV: -3.483e+01\tQL: 2.553e+01\tPR: 8.432\tNR: -104.445\tR: 3.987\tLR: -100.000\n","Epi: 425\tSteps: 79\tQV: -3.478e+01\tQL: 2.358e+01\tPR: 0.000\tNR: -115.991\tR: -15.991\tLR: -100.000\n","Epi: 426\tSteps: 52\tQV: -3.485e+01\tQL: 2.367e+01\tPR: 3.247\tNR: -104.661\tR: -1.414\tLR: -100.000\n","Epi: 427\tSteps: 53\tQV: -3.486e+01\tQL: 2.348e+01\tPR: 1.478\tNR: -103.991\tR: -2.514\tLR: -100.000\n","Epi: 428\tSteps: 116\tQV: -3.484e+01\tQL: 2.277e+01\tPR: 1.902\tNR: -117.176\tR: -15.274\tLR: -100.000\n","Epi: 429\tSteps: 86\tQV: -3.486e+01\tQL: 2.375e+01\tPR: 7.026\tNR: -104.921\tR: 2.105\tLR: -100.000\n","Epi: 430\tSteps: 60\tQV: -3.486e+01\tQL: 2.275e+01\tPR: 4.265\tNR: -105.471\tR: -1.206\tLR: -100.000\n","Epi: 431\tSteps: 60\tQV: -3.494e+01\tQL: 2.516e+01\tPR: 3.075\tNR: -104.501\tR: -1.425\tLR: -100.000\n","Epi: 432\tSteps: 74\tQV: -3.489e+01\tQL: 2.319e+01\tPR: 0.480\tNR: -116.155\tR: -15.675\tLR: -100.000\n","Epi: 433\tSteps: 50\tQV: -3.492e+01\tQL: 2.359e+01\tPR: 3.723\tNR: -104.538\tR: -0.814\tLR: -100.000\n","Epi: 434\tSteps: 55\tQV: -3.495e+01\tQL: 2.464e+01\tPR: 3.138\tNR: -105.487\tR: -2.349\tLR: -100.000\n","Epi: 435\tSteps: 99\tQV: -3.500e+01\tQL: 2.679e+01\tPR: 10.617\tNR: -105.907\tR: 4.710\tLR: -100.000\n","Epi: 436\tSteps: 68\tQV: -3.510e+01\tQL: 3.003e+01\tPR: 4.869\tNR: -104.183\tR: 0.686\tLR: -100.000\n","Epi: 437\tSteps: 66\tQV: -3.493e+01\tQL: 2.712e+01\tPR: 5.312\tNR: -106.726\tR: -1.414\tLR: -100.000\n","Epi: 438\tSteps: 65\tQV: -3.500e+01\tQL: 2.461e+01\tPR: 5.845\tNR: -103.669\tR: 2.176\tLR: -100.000\n","Epi: 439\tSteps: 52\tQV: -3.500e+01\tQL: 2.435e+01\tPR: 2.895\tNR: -104.305\tR: -1.409\tLR: -100.000\n","Epi: 440\tSteps: 81\tQV: -3.491e+01\tQL: 2.064e+01\tPR: 6.378\tNR: -104.729\tR: 1.649\tLR: -100.000\n","Epi: 441\tSteps: 52\tQV: -3.495e+01\tQL: 2.177e+01\tPR: 4.166\tNR: -105.147\tR: -0.981\tLR: -100.000\n","Epi: 442\tSteps: 122\tQV: -3.503e+01\tQL: 2.297e+01\tPR: 7.230\tNR: -109.741\tR: -2.511\tLR: -100.000\n","Epi: 443\tSteps: 65\tQV: -3.512e+01\tQL: 2.338e+01\tPR: 5.040\tNR: -105.674\tR: -0.633\tLR: -100.000\n","Epi: 444\tSteps: 91\tQV: -3.509e+01\tQL: 2.781e+01\tPR: 6.730\tNR: -105.497\tR: 1.233\tLR: -100.000\n","Epi: 445\tSteps: 89\tQV: -3.509e+01\tQL: 2.696e+01\tPR: 1.908\tNR: -119.546\tR: -17.638\tLR: -100.000\n","Epi: 446\tSteps: 63\tQV: -3.507e+01\tQL: 2.125e+01\tPR: 5.195\tNR: -105.411\tR: -0.216\tLR: -100.000\n","Epi: 447\tSteps: 72\tQV: -3.527e+01\tQL: 3.320e+01\tPR: 5.433\tNR: -105.618\tR: -0.185\tLR: -100.000\n","Epi: 448\tSteps: 91\tQV: -3.514e+01\tQL: 2.626e+01\tPR: 0.222\tNR: -121.177\tR: -20.955\tLR: -100.000\n","Epi: 449\tSteps: 58\tQV: -3.515e+01\tQL: 2.688e+01\tPR: 3.626\tNR: -106.678\tR: -3.052\tLR: -100.000\n","Epi: 450\tSteps: 70\tQV: -3.511e+01\tQL: 2.186e+01\tPR: 3.420\tNR: -107.577\tR: -4.157\tLR: -100.000\n","Epi: 451\tSteps: 73\tQV: -3.506e+01\tQL: 2.151e+01\tPR: 4.409\tNR: -105.929\tR: -1.520\tLR: -100.000\n","Epi: 452\tSteps: 71\tQV: -3.515e+01\tQL: 2.554e+01\tPR: 5.255\tNR: -103.485\tR: 1.771\tLR: -100.000\n","Epi: 453\tSteps: 48\tQV: -3.515e+01\tQL: 2.636e+01\tPR: 0.472\tNR: -108.440\tR: -7.968\tLR: -100.000\n","Epi: 454\tSteps: 66\tQV: -3.526e+01\tQL: 2.538e+01\tPR: 4.624\tNR: -103.520\tR: 1.103\tLR: -100.000\n","Epi: 455\tSteps: 70\tQV: -3.521e+01\tQL: 2.425e+01\tPR: 0.926\tNR: -110.444\tR: -9.519\tLR: -100.000\n","Epi: 456\tSteps: 44\tQV: -3.525e+01\tQL: 2.507e+01\tPR: 0.124\tNR: -105.402\tR: -5.278\tLR: -100.000\n","Epi: 457\tSteps: 64\tQV: -3.517e+01\tQL: 2.241e+01\tPR: 0.612\tNR: -107.459\tR: -6.846\tLR: -100.000\n","Epi: 458\tSteps: 53\tQV: -3.518e+01\tQL: 2.420e+01\tPR: 0.138\tNR: -119.658\tR: -19.520\tLR: -100.000\n","Epi: 459\tSteps: 80\tQV: -3.531e+01\tQL: 2.566e+01\tPR: 6.707\tNR: -103.673\tR: 3.034\tLR: -100.000\n","Epi: 460\tSteps: 74\tQV: -3.533e+01\tQL: 2.506e+01\tPR: 4.506\tNR: -106.836\tR: -2.330\tLR: -100.000\n","Epi: 461\tSteps: 1077\tQV: -3.540e+01\tQL: 2.616e+01\tPR: 23.383\tNR: -185.820\tR: -62.437\tLR: -100.000\n","Epi: 462\tSteps: 64\tQV: -3.558e+01\tQL: 2.684e+01\tPR: 5.290\tNR: -102.591\tR: 2.699\tLR: -100.000\n","Epi: 463\tSteps: 1600\tQV: -3.562e+01\tQL: 2.390e+01\tPR: 8.733\tNR: -113.294\tR: -104.469\tLR: -0.093\n","Epi: 464\tSteps: 123\tQV: -3.578e+01\tQL: 2.642e+01\tPR: 7.825\tNR: -109.077\tR: -1.251\tLR: -100.000\n","Epi: 465\tSteps: 46\tQV: -3.572e+01\tQL: 2.022e+01\tPR: 0.256\tNR: -104.453\tR: -4.197\tLR: -100.000\n","Epi: 466\tSteps: 45\tQV: -3.588e+01\tQL: 2.818e+01\tPR: 0.095\tNR: -105.245\tR: -5.150\tLR: -100.000\n","Epi: 467\tSteps: 51\tQV: -3.589e+01\tQL: 2.877e+01\tPR: 1.043\tNR: -104.934\tR: -3.891\tLR: -100.000\n","Epi: 468\tSteps: 1600\tQV: -3.598e+01\tQL: 2.611e+01\tPR: 4.466\tNR: -131.402\tR: -126.828\tLR: -0.108\n","Epi: 469\tSteps: 1600\tQV: -3.624e+01\tQL: 2.479e+01\tPR: 10.834\tNR: -112.944\tR: -102.018\tLR: -0.092\n","Epi: 470\tSteps: 1600\tQV: -3.649e+01\tQL: 2.450e+01\tPR: 7.430\tNR: -109.991\tR: -102.373\tLR: -0.187\n","Epi: 471\tSteps: 1600\tQV: -3.669e+01\tQL: 2.285e+01\tPR: 12.417\tNR: -122.263\tR: -109.824\tLR: -0.022\n","Epi: 472\tSteps: 1600\tQV: -3.691e+01\tQL: 2.432e+01\tPR: 6.786\tNR: -130.603\tR: -123.743\tLR: -0.074\n","Epi: 473\tSteps: 1600\tQV: -3.706e+01\tQL: 2.267e+01\tPR: 7.858\tNR: -126.252\tR: -118.292\tLR: -0.102\n","Epi: 474\tSteps: 1600\tQV: -3.722e+01\tQL: 2.243e+01\tPR: 6.247\tNR: -123.936\tR: -117.621\tLR: -0.068\n","Epi: 475\tSteps: 43\tQV: -3.734e+01\tQL: 2.421e+01\tPR: 0.000\tNR: -116.491\tR: -16.491\tLR: -100.000\n","Epi: 476\tSteps: 1600\tQV: -3.737e+01\tQL: 2.206e+01\tPR: 8.978\tNR: -131.956\tR: -122.865\tLR: -0.114\n","Epi: 477\tSteps: 1600\tQV: -3.749e+01\tQL: 2.132e+01\tPR: 9.197\tNR: -129.471\tR: -120.041\tLR: -0.233\n","Epi: 478\tSteps: 1600\tQV: -3.765e+01\tQL: 2.223e+01\tPR: 16.806\tNR: -127.736\tR: -111.049\tLR: 0.119\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0tF0aR6gP5xp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"status":"error","timestamp":1592835149968,"user_tz":-330,"elapsed":1907,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}},"outputId":"012e8034-3d74-4765-e09e-7e9f58c61cf0"},"source":["print(agent.mem.cntr)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5cda34de094e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcntr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"]}]},{"cell_type":"code","metadata":{"id":"4PN1ufHqkY_X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1592813783415,"user_tz":-330,"elapsed":2406,"user":{"displayName":"KAVITA WAGH","photoUrl":"","userId":"05618127831816058154"}}},"source":["#show_video()\n"],"execution_count":31,"outputs":[]}]}